{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "A couple of months ago Keras added CuDNNLSTM and CuDNNGRU layers, which are special implementations of the regular LSTM and GRU layers backed by NVIDIA's cuDNN library. This means that if you have access to a CUDA GPU, training recurrent neural networks just got a whole lot faster.\n",
    "\n",
    "In this blogpost I'll be showing a simple implementation of an LSTM network and compare the computations times of a network implemented in the CuDDNLSTM layer with one implemented with the regular LSTM layer. I will also be using this opportunity to explore the PTB Diagnostic ECG Database (https://www.physionet.org/physiobank/database/ptbdb/), a database containing ECG data of 290 subjects of which some have a particular heart disease.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural networks & LSTMs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lnies\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from wfdb import io, plot\n",
    "import wfdb\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout, Input\n",
    "from keras.layers import CuDNNLSTM, LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comments_to_dict(comments):\n",
    "    key_value_pairs = [comment.split(':') for comment in comments]\n",
    "    return {pair[0]: pair[1] for pair in key_value_pairs}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['patient001/s0010_re',\n",
       " 'patient001/s0014lre',\n",
       " 'patient001/s0016lre',\n",
       " 'patient002/s0015lre',\n",
       " 'patient003/s0017lre',\n",
       " 'patient004/s0020are',\n",
       " 'patient004/s0020bre',\n",
       " 'patient005/s0021are',\n",
       " 'patient005/s0021bre',\n",
       " 'patient005/s0025lre',\n",
       " 'patient005/s0031lre',\n",
       " 'patient005/s0101lre',\n",
       " 'patient006/s0022lre',\n",
       " 'patient006/s0027lre',\n",
       " 'patient006/s0064lre',\n",
       " 'patient007/s0026lre',\n",
       " 'patient007/s0029lre',\n",
       " 'patient007/s0038lre',\n",
       " 'patient007/s0078lre',\n",
       " 'patient008/s0028lre',\n",
       " 'patient008/s0037lre',\n",
       " 'patient008/s0068lre',\n",
       " 'patient009/s0035_re',\n",
       " 'patient010/s0036lre',\n",
       " 'patient010/s0042lre',\n",
       " 'patient010/s0061lre',\n",
       " 'patient011/s0039lre',\n",
       " 'patient011/s0044lre',\n",
       " 'patient011/s0049lre',\n",
       " 'patient011/s0067lre',\n",
       " 'patient012/s0043lre',\n",
       " 'patient012/s0050lre',\n",
       " 'patient013/s0045lre',\n",
       " 'patient013/s0051lre',\n",
       " 'patient013/s0072lre',\n",
       " 'patient014/s0046lre',\n",
       " 'patient014/s0056lre',\n",
       " 'patient014/s0071lre',\n",
       " 'patient015/s0047lre',\n",
       " 'patient015/s0057lre',\n",
       " 'patient015/s0152lre',\n",
       " 'patient016/s0052lre',\n",
       " 'patient016/s0060lre',\n",
       " 'patient016/s0076lre',\n",
       " 'patient017/s0053lre',\n",
       " 'patient017/s0055lre',\n",
       " 'patient017/s0063lre',\n",
       " 'patient017/s0075lre',\n",
       " 'patient018/s0054lre',\n",
       " 'patient018/s0059lre',\n",
       " 'patient018/s0082lre',\n",
       " 'patient019/s0058lre',\n",
       " 'patient019/s0070lre',\n",
       " 'patient019/s0077lre',\n",
       " 'patient020/s0062lre',\n",
       " 'patient020/s0069lre',\n",
       " 'patient020/s0079lre',\n",
       " 'patient021/s0065lre',\n",
       " 'patient021/s0073lre',\n",
       " 'patient021/s0097lre',\n",
       " 'patient022/s0066lre',\n",
       " 'patient022/s0074lre',\n",
       " 'patient022/s0149lre',\n",
       " 'patient023/s0080lre',\n",
       " 'patient023/s0081lre',\n",
       " 'patient023/s0085lre',\n",
       " 'patient023/s0103lre',\n",
       " 'patient024/s0083lre',\n",
       " 'patient024/s0084lre',\n",
       " 'patient024/s0086lre',\n",
       " 'patient024/s0094lre',\n",
       " 'patient025/s0087lre',\n",
       " 'patient025/s0091lre',\n",
       " 'patient025/s0150lre',\n",
       " 'patient026/s0088lre',\n",
       " 'patient026/s0095lre',\n",
       " 'patient027/s0089lre',\n",
       " 'patient027/s0096lre',\n",
       " 'patient027/s0151lre',\n",
       " 'patient028/s0090lre',\n",
       " 'patient028/s0093lre',\n",
       " 'patient028/s0108lre',\n",
       " 'patient029/s0092lre',\n",
       " 'patient029/s0098lre',\n",
       " 'patient029/s0122lre',\n",
       " 'patient030/s0099lre',\n",
       " 'patient030/s0107lre',\n",
       " 'patient030/s0117lre',\n",
       " 'patient030/s0153lre',\n",
       " 'patient031/s0100lre',\n",
       " 'patient031/s0104lre',\n",
       " 'patient031/s0114lre',\n",
       " 'patient031/s0127lre',\n",
       " 'patient032/s0102lre',\n",
       " 'patient032/s0106lre',\n",
       " 'patient032/s0115lre',\n",
       " 'patient032/s0165lre',\n",
       " 'patient033/s0105lre',\n",
       " 'patient033/s0113lre',\n",
       " 'patient033/s0121lre',\n",
       " 'patient033/s0157lre',\n",
       " 'patient034/s0109lre',\n",
       " 'patient034/s0118lre',\n",
       " 'patient034/s0123lre',\n",
       " 'patient034/s0158lre',\n",
       " 'patient035/s0110lre',\n",
       " 'patient035/s0119lre',\n",
       " 'patient035/s0124lre',\n",
       " 'patient035/s0145lre',\n",
       " 'patient036/s0111lre',\n",
       " 'patient036/s0116lre',\n",
       " 'patient036/s0126lre',\n",
       " 'patient037/s0112lre',\n",
       " 'patient037/s0120lre',\n",
       " 'patient038/s0125lre',\n",
       " 'patient038/s0128lre',\n",
       " 'patient038/s0162lre',\n",
       " 'patient039/s0129lre',\n",
       " 'patient039/s0134lre',\n",
       " 'patient039/s0164lre',\n",
       " 'patient040/s0130lre',\n",
       " 'patient040/s0131lre',\n",
       " 'patient040/s0133lre',\n",
       " 'patient040/s0219lre',\n",
       " 'patient041/s0132lre',\n",
       " 'patient041/s0136lre',\n",
       " 'patient041/s0138lre',\n",
       " 'patient041/s0276lre',\n",
       " 'patient042/s0135lre',\n",
       " 'patient042/s0137lre',\n",
       " 'patient042/s0140lre',\n",
       " 'patient042/s0347lre',\n",
       " 'patient043/s0141lre',\n",
       " 'patient043/s0144lre',\n",
       " 'patient043/s0278lre',\n",
       " 'patient044/s0142lre',\n",
       " 'patient044/s0143lre',\n",
       " 'patient044/s0146lre',\n",
       " 'patient044/s0159lre',\n",
       " 'patient045/s0147lre',\n",
       " 'patient045/s0148lre',\n",
       " 'patient045/s0155lre',\n",
       " 'patient045/s0217lre',\n",
       " 'patient046/s0156lre',\n",
       " 'patient046/s0161lre',\n",
       " 'patient046/s0168lre',\n",
       " 'patient046/s0184lre',\n",
       " 'patient047/s0160lre',\n",
       " 'patient047/s0163lre',\n",
       " 'patient047/s0167lre',\n",
       " 'patient048/s0171lre',\n",
       " 'patient048/s0172lre',\n",
       " 'patient048/s0180lre',\n",
       " 'patient048/s0277lre',\n",
       " 'patient049/s0173lre',\n",
       " 'patient049/s0178lre',\n",
       " 'patient049/s0186lre',\n",
       " 'patient049/s0314lre',\n",
       " 'patient050/s0174lre',\n",
       " 'patient050/s0177lre',\n",
       " 'patient050/s0185lre',\n",
       " 'patient050/s0215lre',\n",
       " 'patient051/s0179lre',\n",
       " 'patient051/s0181lre',\n",
       " 'patient051/s0187lre',\n",
       " 'patient051/s0213lre',\n",
       " 'patient052/s0190lre',\n",
       " 'patient053/s0191lre',\n",
       " 'patient054/s0192lre',\n",
       " 'patient054/s0195lre',\n",
       " 'patient054/s0197lre',\n",
       " 'patient054/s0218lre',\n",
       " 'patient055/s0194lre',\n",
       " 'patient056/s0196lre',\n",
       " 'patient057/s0198lre',\n",
       " 'patient058/s0216lre',\n",
       " 'patient059/s0208lre',\n",
       " 'patient060/s0209lre',\n",
       " 'patient061/s0210lre',\n",
       " 'patient062/s0212lre',\n",
       " 'patient063/s0214lre',\n",
       " 'patient064/s0220lre',\n",
       " 'patient065/s0221lre',\n",
       " 'patient065/s0226lre',\n",
       " 'patient065/s0229lre',\n",
       " 'patient065/s0282lre',\n",
       " 'patient066/s0225lre',\n",
       " 'patient066/s0231lre',\n",
       " 'patient066/s0280lre',\n",
       " 'patient067/s0227lre',\n",
       " 'patient067/s0230lre',\n",
       " 'patient067/s0283lre',\n",
       " 'patient068/s0228lre',\n",
       " 'patient069/s0232lre',\n",
       " 'patient069/s0233lre',\n",
       " 'patient069/s0234lre',\n",
       " 'patient069/s0284lre',\n",
       " 'patient070/s0235lre',\n",
       " 'patient071/s0236lre',\n",
       " 'patient072/s0237lre',\n",
       " 'patient072/s0240lre',\n",
       " 'patient072/s0244lre',\n",
       " 'patient072/s0318lre',\n",
       " 'patient073/s0238lre',\n",
       " 'patient073/s0243lre',\n",
       " 'patient073/s0249lre',\n",
       " 'patient073/s0252lre',\n",
       " 'patient074/s0239lre',\n",
       " 'patient074/s0241lre',\n",
       " 'patient074/s0245lre',\n",
       " 'patient074/s0406lre',\n",
       " 'patient075/s0242lre',\n",
       " 'patient075/s0246lre',\n",
       " 'patient075/s0248lre',\n",
       " 'patient075/s0327lre',\n",
       " 'patient076/s0247lre',\n",
       " 'patient076/s0250lre',\n",
       " 'patient076/s0253lre',\n",
       " 'patient076/s0319lre',\n",
       " 'patient077/s0251lre',\n",
       " 'patient077/s0254lre',\n",
       " 'patient077/s0258lre',\n",
       " 'patient077/s0285lre',\n",
       " 'patient078/s0255lre',\n",
       " 'patient078/s0259lre',\n",
       " 'patient078/s0262lre',\n",
       " 'patient078/s0317lre',\n",
       " 'patient079/s0256lre',\n",
       " 'patient079/s0257lre',\n",
       " 'patient079/s0263lre',\n",
       " 'patient079/s0269lre',\n",
       " 'patient080/s0260lre',\n",
       " 'patient080/s0261lre',\n",
       " 'patient080/s0265lre',\n",
       " 'patient080/s0315lre',\n",
       " 'patient081/s0264lre',\n",
       " 'patient081/s0266lre',\n",
       " 'patient081/s0270lre',\n",
       " 'patient081/s0346lre',\n",
       " 'patient082/s0267lre',\n",
       " 'patient082/s0271lre',\n",
       " 'patient082/s0279lre',\n",
       " 'patient082/s0320lre',\n",
       " 'patient083/s0268lre',\n",
       " 'patient083/s0272lre',\n",
       " 'patient083/s0286lre',\n",
       " 'patient083/s0290lre',\n",
       " 'patient084/s0281lre',\n",
       " 'patient084/s0288lre',\n",
       " 'patient084/s0289lre',\n",
       " 'patient084/s0313lre',\n",
       " 'patient085/s0296lre',\n",
       " 'patient085/s0297lre',\n",
       " 'patient085/s0298lre',\n",
       " 'patient085/s0345lre',\n",
       " 'patient086/s0316lre',\n",
       " 'patient087/s0321lre',\n",
       " 'patient087/s0326lre',\n",
       " 'patient087/s0330lre',\n",
       " 'patient088/s0339lre',\n",
       " 'patient088/s0343lre',\n",
       " 'patient088/s0352lre',\n",
       " 'patient088/s0413lre',\n",
       " 'patient089/s0344lre',\n",
       " 'patient089/s0355lre',\n",
       " 'patient089/s0359lre',\n",
       " 'patient089/s0372lre',\n",
       " 'patient090/s0348lre',\n",
       " 'patient090/s0356lre',\n",
       " 'patient090/s0360lre',\n",
       " 'patient090/s0418lre',\n",
       " 'patient091/s0353lre',\n",
       " 'patient091/s0357lre',\n",
       " 'patient091/s0361lre',\n",
       " 'patient091/s0408lre',\n",
       " 'patient092/s0354lre',\n",
       " 'patient092/s0358lre',\n",
       " 'patient092/s0362lre',\n",
       " 'patient092/s0411lre',\n",
       " 'patient093/s0367lre',\n",
       " 'patient093/s0371lre',\n",
       " 'patient093/s0375lre',\n",
       " 'patient093/s0378lre',\n",
       " 'patient093/s0396lre',\n",
       " 'patient094/s0368lre',\n",
       " 'patient094/s0370lre',\n",
       " 'patient094/s0376lre',\n",
       " 'patient094/s0412lre',\n",
       " 'patient095/s0369lre',\n",
       " 'patient095/s0373lre',\n",
       " 'patient095/s0377lre',\n",
       " 'patient095/s0417lre',\n",
       " 'patient096/s0379lre',\n",
       " 'patient096/s0381lre',\n",
       " 'patient096/s0385lre',\n",
       " 'patient096/s0395lre',\n",
       " 'patient097/s0380lre',\n",
       " 'patient097/s0382lre',\n",
       " 'patient097/s0384lre',\n",
       " 'patient097/s0394lre',\n",
       " 'patient098/s0386lre',\n",
       " 'patient098/s0389lre',\n",
       " 'patient098/s0398lre',\n",
       " 'patient098/s0409lre',\n",
       " 'patient099/s0387lre',\n",
       " 'patient099/s0388lre',\n",
       " 'patient099/s0397lre',\n",
       " 'patient099/s0419lre',\n",
       " 'patient100/s0399lre',\n",
       " 'patient100/s0401lre',\n",
       " 'patient100/s0407lre',\n",
       " 'patient101/s0400lre',\n",
       " 'patient101/s0410lre',\n",
       " 'patient101/s0414lre',\n",
       " 'patient102/s0416lre',\n",
       " 'patient103/s0332lre',\n",
       " 'patient104/s0306lre',\n",
       " 'patient105/s0303lre',\n",
       " 'patient106/s0030_re',\n",
       " 'patient107/s0199_re',\n",
       " 'patient108/s0013_re',\n",
       " 'patient109/s0349lre',\n",
       " 'patient110/s0003_re',\n",
       " 'patient111/s0203_re',\n",
       " 'patient112/s0169_re',\n",
       " 'patient113/s0018cre',\n",
       " 'patient113/s0018lre',\n",
       " 'patient114/s0012_re',\n",
       " 'patient115/s0023_re',\n",
       " 'patient116/s0302lre',\n",
       " 'patient117/s0291lre',\n",
       " 'patient117/s0292lre',\n",
       " 'patient118/s0183_re',\n",
       " 'patient119/s0001_re',\n",
       " 'patient120/s0331lre',\n",
       " 'patient121/s0311lre',\n",
       " 'patient122/s0312lre',\n",
       " 'patient123/s0224_re',\n",
       " 'patient125/s0006_re',\n",
       " 'patient126/s0154_re',\n",
       " 'patient127/s0342lre',\n",
       " 'patient127/s0383lre',\n",
       " 'patient128/s0182_re',\n",
       " 'patient129/s0189_re',\n",
       " 'patient130/s0166_re',\n",
       " 'patient131/s0273lre',\n",
       " 'patient133/s0393lre',\n",
       " 'patient135/s0334lre',\n",
       " 'patient136/s0205_re',\n",
       " 'patient137/s0392lre',\n",
       " 'patient138/s0005_re',\n",
       " 'patient139/s0223_re',\n",
       " 'patient140/s0019_re',\n",
       " 'patient141/s0307lre',\n",
       " 'patient142/s0351lre',\n",
       " 'patient143/s0333lre',\n",
       " 'patient144/s0341lre',\n",
       " 'patient145/s0201_re',\n",
       " 'patient146/s0007_re',\n",
       " 'patient147/s0211_re',\n",
       " 'patient148/s0335lre',\n",
       " 'patient149/s0202are',\n",
       " 'patient149/s0202bre',\n",
       " 'patient150/s0287lre',\n",
       " 'patient151/s0206_re',\n",
       " 'patient152/s0004_re',\n",
       " 'patient153/s0391lre',\n",
       " 'patient154/s0170_re',\n",
       " 'patient155/s0301lre',\n",
       " 'patient156/s0299lre',\n",
       " 'patient157/s0338lre',\n",
       " 'patient158/s0294lre',\n",
       " 'patient158/s0295lre',\n",
       " 'patient159/s0390lre',\n",
       " 'patient160/s0222_re',\n",
       " 'patient162/s0193_re',\n",
       " 'patient163/s0034_re',\n",
       " 'patient164/s0024are',\n",
       " 'patient164/s0024bre',\n",
       " 'patient165/s0322lre',\n",
       " 'patient165/s0323lre',\n",
       " 'patient166/s0275lre',\n",
       " 'patient167/s0200_re',\n",
       " 'patient168/s0032_re',\n",
       " 'patient168/s0033_re',\n",
       " 'patient169/s0328lre',\n",
       " 'patient169/s0329lre',\n",
       " 'patient170/s0274lre',\n",
       " 'patient171/s0364lre',\n",
       " 'patient172/s0304lre',\n",
       " 'patient173/s0305lre',\n",
       " 'patient174/s0300lre',\n",
       " 'patient174/s0324lre',\n",
       " 'patient174/s0325lre',\n",
       " 'patient175/s0009_re',\n",
       " 'patient176/s0188_re',\n",
       " 'patient177/s0366lre',\n",
       " 'patient178/s0011_re',\n",
       " 'patient179/s0176_re',\n",
       " 'patient180/s0374lre',\n",
       " 'patient180/s0475_re',\n",
       " 'patient180/s0476_re',\n",
       " 'patient180/s0477_re',\n",
       " 'patient180/s0490_re',\n",
       " 'patient180/s0545_re',\n",
       " 'patient180/s0561_re',\n",
       " 'patient181/s0204are',\n",
       " 'patient181/s0204bre',\n",
       " 'patient182/s0308lre',\n",
       " 'patient183/s0175_re',\n",
       " 'patient184/s0363lre',\n",
       " 'patient185/s0336lre',\n",
       " 'patient186/s0293lre',\n",
       " 'patient187/s0207_re',\n",
       " 'patient188/s0365lre',\n",
       " 'patient189/s0309lre',\n",
       " 'patient190/s0040_re',\n",
       " 'patient190/s0041_re',\n",
       " 'patient191/s0340lre',\n",
       " 'patient192/s0048_re',\n",
       " 'patient193/s0008_re',\n",
       " 'patient194/s0310lre',\n",
       " 'patient195/s0337lre',\n",
       " 'patient196/s0002_re',\n",
       " 'patient197/s0350lre',\n",
       " 'patient197/s0403lre',\n",
       " 'patient198/s0402lre',\n",
       " 'patient198/s0415lre',\n",
       " 'patient199/s0404lre',\n",
       " 'patient200/s0405lre',\n",
       " 'patient201/s0420_re',\n",
       " 'patient201/s0423_re',\n",
       " 'patient202/s0421_re',\n",
       " 'patient202/s0422_re',\n",
       " 'patient203/s0424_re',\n",
       " 'patient204/s0425_re',\n",
       " 'patient205/s0426_re',\n",
       " 'patient206/s0427_re',\n",
       " 'patient207/s0428_re',\n",
       " 'patient208/s0429_re',\n",
       " 'patient208/s0430_re',\n",
       " 'patient209/s0431_re',\n",
       " 'patient210/s0432_re',\n",
       " 'patient211/s0433_re',\n",
       " 'patient212/s0434_re',\n",
       " 'patient213/s0435_re',\n",
       " 'patient214/s0436_re',\n",
       " 'patient215/s0437_re',\n",
       " 'patient216/s0438_re',\n",
       " 'patient217/s0439_re',\n",
       " 'patient218/s0440_re',\n",
       " 'patient219/s0441_re',\n",
       " 'patient220/s0442_re',\n",
       " 'patient221/s0443_re',\n",
       " 'patient222/s0444_re',\n",
       " 'patient223/s0445_re',\n",
       " 'patient223/s0446_re',\n",
       " 'patient224/s0447_re',\n",
       " 'patient225/s0448_re',\n",
       " 'patient226/s0449_re',\n",
       " 'patient227/s0450_re',\n",
       " 'patient228/s0451_re',\n",
       " 'patient229/s0452_re',\n",
       " 'patient229/s0453_re',\n",
       " 'patient230/s0454_re',\n",
       " 'patient231/s0455_re',\n",
       " 'patient232/s0456_re',\n",
       " 'patient233/s0457_re',\n",
       " 'patient233/s0458_re',\n",
       " 'patient233/s0459_re',\n",
       " 'patient233/s0482_re',\n",
       " 'patient233/s0483_re',\n",
       " 'patient234/s0460_re',\n",
       " 'patient235/s0461_re',\n",
       " 'patient236/s0462_re',\n",
       " 'patient236/s0463_re',\n",
       " 'patient236/s0464_re',\n",
       " 'patient237/s0465_re',\n",
       " 'patient238/s0466_re',\n",
       " 'patient239/s0467_re',\n",
       " 'patient240/s0468_re',\n",
       " 'patient241/s0469_re',\n",
       " 'patient241/s0470_re',\n",
       " 'patient242/s0471_re',\n",
       " 'patient243/s0472_re',\n",
       " 'patient244/s0473_re',\n",
       " 'patient245/s0474_re',\n",
       " 'patient245/s0480_re',\n",
       " 'patient246/s0478_re',\n",
       " 'patient247/s0479_re',\n",
       " 'patient248/s0481_re',\n",
       " 'patient249/s0484_re',\n",
       " 'patient250/s0485_re',\n",
       " 'patient251/s0486_re',\n",
       " 'patient251/s0503_re',\n",
       " 'patient251/s0506_re',\n",
       " 'patient252/s0487_re',\n",
       " 'patient253/s0488_re',\n",
       " 'patient254/s0489_re',\n",
       " 'patient255/s0491_re',\n",
       " 'patient256/s0492_re',\n",
       " 'patient257/s0493_re',\n",
       " 'patient258/s0494_re',\n",
       " 'patient259/s0495_re',\n",
       " 'patient260/s0496_re',\n",
       " 'patient261/s0497_re',\n",
       " 'patient262/s0498_re',\n",
       " 'patient263/s0499_re',\n",
       " 'patient264/s0500_re',\n",
       " 'patient265/s0501_re',\n",
       " 'patient266/s0502_re',\n",
       " 'patient267/s0504_re',\n",
       " 'patient268/s0505_re',\n",
       " 'patient269/s0508_re',\n",
       " 'patient270/s0507_re',\n",
       " 'patient271/s0509_re',\n",
       " 'patient272/s0510_re',\n",
       " 'patient273/s0511_re',\n",
       " 'patient274/s0512_re',\n",
       " 'patient275/s0513_re',\n",
       " 'patient276/s0526_re',\n",
       " 'patient277/s0527_re',\n",
       " 'patient278/s0528_re',\n",
       " 'patient278/s0529_re',\n",
       " 'patient278/s0530_re',\n",
       " 'patient279/s0531_re',\n",
       " 'patient279/s0532_re',\n",
       " 'patient279/s0533_re',\n",
       " 'patient279/s0534_re',\n",
       " 'patient280/s0535_re',\n",
       " 'patient281/s0537_re',\n",
       " 'patient282/s0539_re',\n",
       " 'patient283/s0542_re',\n",
       " 'patient284/s0543_re',\n",
       " 'patient284/s0551_re',\n",
       " 'patient284/s0552_re',\n",
       " 'patient285/s0544_re',\n",
       " 'patient286/s0546_re',\n",
       " 'patient287/s0547_re',\n",
       " 'patient287/s0548_re',\n",
       " 'patient288/s0549_re',\n",
       " 'patient289/s0550_re',\n",
       " 'patient290/s0553_re',\n",
       " 'patient291/s0554_re',\n",
       " 'patient292/s0555_re',\n",
       " 'patient292/s0556_re',\n",
       " 'patient293/s0557_re',\n",
       " 'patient293/s0558_re',\n",
       " 'patient294/s0559_re']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data'\n",
    "db = 'ptbdb'\n",
    "record_names = io.get_record_list(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_to_row(record, patient_id):\n",
    "    row = {}\n",
    "    row['patient'] = patient_id\n",
    "    row['name'] = record.record_name\n",
    "    row['label'] = comments_to_dict(record.comments)['Reason for admission'][1:]\n",
    "    row['signals'] = record.p_signal\n",
    "    row['signal_length'] = record.sig_len\n",
    "    channels = record.sig_name\n",
    "    signals = record.p_signal.transpose()\n",
    "    \n",
    "    row['channels'] = channels\n",
    "    \n",
    "    for channel, signal in zip(channels, signals):\n",
    "        row[channel] = signal\n",
    "        \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70523d953283407284bad3627cc6a8f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=549), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "for record_name in tqdm(record_names):\n",
    "    record = io.rdrecord(record_name=os.path.join('data', record_name))\n",
    "    label = comments_to_dict(record.comments)['Reason for admission'][1:]\n",
    "    patient = record_name.split('/')[0]\n",
    "    signal_length = record.sig_len\n",
    "    records.append({'name':record_name, 'label':label, 'patient':patient, 'signal_length':signal_length})\n",
    "    \n",
    "channels = record.sig_name\n",
    "df_records = pd.DataFrame(records)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Myocardial infarction     368\n",
       "Healthy control            80\n",
       "n/a                        27\n",
       "Bundle branch block        17\n",
       "Cardiomyopathy             17\n",
       "Dysrhythmia                16\n",
       "Hypertrophy                 7\n",
       "Valvular heart disease      6\n",
       "Myocarditis                 4\n",
       "Stable angina               2\n",
       "Heart failure (NYHA 3)      1\n",
       "Heart failure (NYHA 2)      1\n",
       "Unstable angina             1\n",
       "Palpitation                 1\n",
       "Heart failure (NYHA 4)      1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df_records['label'].unique()\n",
    "df_records['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_labels = [\n",
    "    'Healthy control',\n",
    "    'Myocardial infarction'\n",
    "    ]\n",
    "df_selected = df_records.loc[df_records['label'].isin(selected_labels)]\n",
    "label_map = {label: value for label, value in zip(selected_labels, range(len(selected_labels)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_patients = []\n",
    "train_patients = []\n",
    "test_size = 0.2\n",
    "channels\n",
    "for label in selected_labels:\n",
    "    df_selected = df_records.loc[df_records['label'] == label]\n",
    "    patients = df_selected['patient'].unique()\n",
    "    n_test = math.ceil(len(patients)*test_size)\n",
    "    test_patients+=list(np.random.choice(patients, n_test, replace=False))\n",
    "    train_patients+=list(patients[np.isin(patients, test_patients, invert=True)])\n",
    "    \n",
    "#df_selected['patient'].sample(len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_set(df_data, channels, label_map, record_id, window_size=2048):\n",
    "    n_windows = 0\n",
    "    \n",
    "    for _, record in tqdm(df_data.iterrows()):\n",
    "        n_windows+= record['signal_length']//window_size\n",
    "\n",
    "    dataX = np.zeros((n_windows, len(channels), window_size))\n",
    "    dataY = np.zeros((n_windows, len(label_map)))\n",
    "    \n",
    "    record_list = []\n",
    "    \n",
    "    nth_window = 0\n",
    "    for i, (patient, record) in enumerate(tqdm(df_data.iterrows())):\n",
    "        # read the record, get the signal data and transpose it\n",
    "        signal_data = io.rdrecord(os.path.join('data', record['name'])).p_signal.transpose()\n",
    "        n_rows = signal_data.shape[-1]\n",
    "        n_windows = n_rows//window_size\n",
    "        dataX[nth_window:nth_window+n_windows] = np.array([signal_data[:,i*window_size:(i+1)*window_size] for i in range(n_windows)])\n",
    "        dataY[nth_window:nth_window+n_windows][:, label_map[record.label]] = 1\n",
    "        nth_window+=n_windows\n",
    "        \n",
    "        if record_id:\n",
    "            record_list+= n_windows*[record['name']]\n",
    "        \n",
    "    return dataX, dataY, record_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6892f6a7f844fbb7e05b02ce9de4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4aaac37dc7e49fc889b450328f7286e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_patient_records = df_records.set_index('patient')\n",
    "df_train_patients = df_patient_records.loc[train_patients]\n",
    "df_test_patients = df_patient_records.loc[test_patients]\n",
    "window_size = 2048#df_records['signal_length'].min()\n",
    "#trainX, trainY, _ = make_set(df_train_patients, channels, label_map, False, window_size)\n",
    "testX, testY, record_list = make_set(df_test_patients, channels, label_map, True, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape, output_dim, lstm_layer, dropout=0.2):\n",
    "    print(\"model dim: \", input_shape, output_dim)\n",
    "    model = Sequential()\n",
    "    model.add(lstm_layer(256, return_sequences=True, input_shape=input_shape, batch_size=None))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(lstm_layer(128, return_sequences=True))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(output_dim, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21aa0f46e45d4d41a016dc57f4e9db69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24bf302f81dd4f58a5606277141befcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3e710b15554882b5c84027ea84f9f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91fa4869f92044bcb603dfb9f7d8cf0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model dim:  (15, 2048) 2\n",
      "Epoch 1/50\n",
      "19910/19910 [==============================] - ETA: 1:57 - loss: 0.212 - ETA: 1:00 - loss: 0.217 - ETA: 41s - loss: 0.213 - ETA: 31s - loss: 0.21 - ETA: 26s - loss: 0.20 - ETA: 22s - loss: 0.21 - ETA: 19s - loss: 0.20 - ETA: 17s - loss: 0.20 - ETA: 15s - loss: 0.20 - ETA: 14s - loss: 0.20 - ETA: 12s - loss: 0.20 - ETA: 11s - loss: 0.20 - ETA: 10s - loss: 0.20 - ETA: 9s - loss: 0.2039 - ETA: 9s - loss: 0.202 - ETA: 8s - loss: 0.201 - ETA: 7s - loss: 0.202 - ETA: 7s - loss: 0.201 - ETA: 6s - loss: 0.200 - ETA: 6s - loss: 0.198 - ETA: 5s - loss: 0.198 - ETA: 5s - loss: 0.197 - ETA: 4s - loss: 0.196 - ETA: 4s - loss: 0.195 - ETA: 4s - loss: 0.194 - ETA: 3s - loss: 0.194 - ETA: 3s - loss: 0.193 - ETA: 3s - loss: 0.193 - ETA: 2s - loss: 0.192 - ETA: 2s - loss: 0.191 - ETA: 2s - loss: 0.190 - ETA: 1s - loss: 0.190 - ETA: 1s - loss: 0.189 - ETA: 1s - loss: 0.188 - ETA: 1s - loss: 0.187 - ETA: 0s - loss: 0.186 - ETA: 0s - loss: 0.186 - ETA: 0s - loss: 0.185 - 10s 512us/step - loss: 0.1853\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.18532, saving model to models\\weights-improvement-01-bigger.hdf5\n",
      "Epoch 2/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.153 - ETA: 6s - loss: 0.149 - ETA: 6s - loss: 0.144 - ETA: 6s - loss: 0.144 - ETA: 5s - loss: 0.140 - ETA: 5s - loss: 0.137 - ETA: 5s - loss: 0.142 - ETA: 5s - loss: 0.143 - ETA: 5s - loss: 0.143 - ETA: 5s - loss: 0.143 - ETA: 4s - loss: 0.143 - ETA: 4s - loss: 0.143 - ETA: 4s - loss: 0.144 - ETA: 4s - loss: 0.144 - ETA: 4s - loss: 0.144 - ETA: 4s - loss: 0.143 - ETA: 3s - loss: 0.144 - ETA: 3s - loss: 0.144 - ETA: 3s - loss: 0.144 - ETA: 3s - loss: 0.144 - ETA: 3s - loss: 0.144 - ETA: 3s - loss: 0.144 - ETA: 2s - loss: 0.144 - ETA: 2s - loss: 0.142 - ETA: 2s - loss: 0.142 - ETA: 2s - loss: 0.142 - ETA: 2s - loss: 0.142 - ETA: 1s - loss: 0.141 - ETA: 1s - loss: 0.141 - ETA: 1s - loss: 0.141 - ETA: 1s - loss: 0.140 - ETA: 1s - loss: 0.140 - ETA: 1s - loss: 0.139 - ETA: 0s - loss: 0.139 - ETA: 0s - loss: 0.139 - ETA: 0s - loss: 0.139 - ETA: 0s - loss: 0.139 - ETA: 0s - loss: 0.139 - 7s 345us/step - loss: 0.1388\n",
      "\n",
      "Epoch 00002: loss improved from 0.18532 to 0.13880, saving model to models\\weights-improvement-02-bigger.hdf5\n",
      "Epoch 3/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.102 - ETA: 6s - loss: 0.113 - ETA: 6s - loss: 0.117 - ETA: 6s - loss: 0.118 - ETA: 5s - loss: 0.117 - ETA: 5s - loss: 0.119 - ETA: 5s - loss: 0.118 - ETA: 5s - loss: 0.119 - ETA: 5s - loss: 0.120 - ETA: 4s - loss: 0.117 - ETA: 4s - loss: 0.117 - ETA: 4s - loss: 0.117 - ETA: 4s - loss: 0.119 - ETA: 4s - loss: 0.118 - ETA: 4s - loss: 0.117 - ETA: 3s - loss: 0.116 - ETA: 3s - loss: 0.116 - ETA: 3s - loss: 0.116 - ETA: 3s - loss: 0.116 - ETA: 3s - loss: 0.116 - ETA: 3s - loss: 0.116 - ETA: 2s - loss: 0.117 - ETA: 2s - loss: 0.117 - ETA: 2s - loss: 0.116 - ETA: 2s - loss: 0.116 - ETA: 2s - loss: 0.116 - ETA: 2s - loss: 0.115 - ETA: 1s - loss: 0.115 - ETA: 1s - loss: 0.114 - ETA: 1s - loss: 0.115 - ETA: 1s - loss: 0.115 - ETA: 1s - loss: 0.114 - ETA: 1s - loss: 0.114 - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.113 - 7s 338us/step - loss: 0.1132\n",
      "\n",
      "Epoch 00003: loss improved from 0.13880 to 0.11318, saving model to models\\weights-improvement-03-bigger.hdf5\n",
      "Epoch 4/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.095 - ETA: 6s - loss: 0.099 - ETA: 6s - loss: 0.097 - ETA: 6s - loss: 0.103 - ETA: 5s - loss: 0.101 - ETA: 5s - loss: 0.102 - ETA: 5s - loss: 0.100 - ETA: 5s - loss: 0.100 - ETA: 5s - loss: 0.101 - ETA: 5s - loss: 0.099 - ETA: 4s - loss: 0.098 - ETA: 4s - loss: 0.096 - ETA: 4s - loss: 0.095 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.093 - ETA: 3s - loss: 0.092 - ETA: 3s - loss: 0.092 - ETA: 3s - loss: 0.092 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.092 - ETA: 2s - loss: 0.091 - ETA: 2s - loss: 0.091 - ETA: 2s - loss: 0.092 - ETA: 2s - loss: 0.092 - ETA: 2s - loss: 0.092 - ETA: 2s - loss: 0.092 - ETA: 1s - loss: 0.092 - ETA: 1s - loss: 0.092 - ETA: 1s - loss: 0.092 - ETA: 1s - loss: 0.092 - ETA: 1s - loss: 0.092 - ETA: 1s - loss: 0.092 - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.091 - 7s 339us/step - loss: 0.0915\n",
      "\n",
      "Epoch 00004: loss improved from 0.11318 to 0.09154, saving model to models\\weights-improvement-04-bigger.hdf5\n",
      "Epoch 5/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.068 - ETA: 6s - loss: 0.066 - ETA: 6s - loss: 0.067 - ETA: 6s - loss: 0.072 - ETA: 5s - loss: 0.075 - ETA: 5s - loss: 0.074 - ETA: 5s - loss: 0.074 - ETA: 5s - loss: 0.074 - ETA: 5s - loss: 0.072 - ETA: 4s - loss: 0.071 - ETA: 4s - loss: 0.075 - ETA: 4s - loss: 0.074 - ETA: 4s - loss: 0.075 - ETA: 4s - loss: 0.075 - ETA: 4s - loss: 0.073 - ETA: 3s - loss: 0.074 - ETA: 3s - loss: 0.074 - ETA: 3s - loss: 0.073 - ETA: 3s - loss: 0.073 - ETA: 3s - loss: 0.073 - ETA: 3s - loss: 0.073 - ETA: 2s - loss: 0.072 - ETA: 2s - loss: 0.073 - ETA: 2s - loss: 0.073 - ETA: 2s - loss: 0.073 - ETA: 2s - loss: 0.072 - ETA: 2s - loss: 0.072 - ETA: 1s - loss: 0.072 - ETA: 1s - loss: 0.072 - ETA: 1s - loss: 0.072 - ETA: 1s - loss: 0.072 - ETA: 1s - loss: 0.072 - ETA: 1s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 7s 339us/step - loss: 0.0725\n",
      "\n",
      "Epoch 00005: loss improved from 0.09154 to 0.07246, saving model to models\\weights-improvement-05-bigger.hdf5\n",
      "Epoch 6/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.050 - ETA: 6s - loss: 0.054 - ETA: 6s - loss: 0.052 - ETA: 6s - loss: 0.053 - ETA: 5s - loss: 0.051 - ETA: 5s - loss: 0.052 - ETA: 5s - loss: 0.051 - ETA: 5s - loss: 0.050 - ETA: 5s - loss: 0.051 - ETA: 4s - loss: 0.051 - ETA: 4s - loss: 0.053 - ETA: 4s - loss: 0.052 - ETA: 4s - loss: 0.053 - ETA: 4s - loss: 0.053 - ETA: 4s - loss: 0.053 - ETA: 3s - loss: 0.055 - ETA: 3s - loss: 0.056 - ETA: 3s - loss: 0.057 - ETA: 3s - loss: 0.057 - ETA: 3s - loss: 0.058 - ETA: 3s - loss: 0.057 - ETA: 2s - loss: 0.058 - ETA: 2s - loss: 0.058 - ETA: 2s - loss: 0.058 - ETA: 2s - loss: 0.058 - ETA: 2s - loss: 0.058 - ETA: 2s - loss: 0.058 - ETA: 1s - loss: 0.058 - ETA: 1s - loss: 0.058 - ETA: 1s - loss: 0.058 - ETA: 1s - loss: 0.058 - ETA: 1s - loss: 0.058 - ETA: 1s - loss: 0.058 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.059 - 7s 339us/step - loss: 0.0598\n",
      "\n",
      "Epoch 00006: loss improved from 0.07246 to 0.05984, saving model to models\\weights-improvement-06-bigger.hdf5\n",
      "Epoch 7/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.048 - ETA: 6s - loss: 0.048 - ETA: 6s - loss: 0.055 - ETA: 5s - loss: 0.057 - ETA: 5s - loss: 0.054 - ETA: 5s - loss: 0.055 - ETA: 5s - loss: 0.055 - ETA: 5s - loss: 0.054 - ETA: 5s - loss: 0.053 - ETA: 5s - loss: 0.053 - ETA: 4s - loss: 0.053 - ETA: 4s - loss: 0.052 - ETA: 4s - loss: 0.052 - ETA: 4s - loss: 0.052 - ETA: 4s - loss: 0.052 - ETA: 3s - loss: 0.051 - ETA: 3s - loss: 0.051 - ETA: 3s - loss: 0.051 - ETA: 3s - loss: 0.051 - ETA: 3s - loss: 0.051 - ETA: 3s - loss: 0.051 - ETA: 2s - loss: 0.051 - ETA: 2s - loss: 0.050 - ETA: 2s - loss: 0.050 - ETA: 2s - loss: 0.050 - ETA: 2s - loss: 0.049 - ETA: 2s - loss: 0.050 - ETA: 1s - loss: 0.050 - ETA: 1s - loss: 0.050 - ETA: 1s - loss: 0.050 - ETA: 1s - loss: 0.050 - ETA: 1s - loss: 0.049 - ETA: 1s - loss: 0.049 - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.048 - ETA: 0s - loss: 0.049 - 7s 338us/step - loss: 0.0494\n",
      "\n",
      "Epoch 00007: loss improved from 0.05984 to 0.04944, saving model to models\\weights-improvement-07-bigger.hdf5\n",
      "Epoch 8/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.045 - ETA: 6s - loss: 0.044 - ETA: 6s - loss: 0.042 - ETA: 6s - loss: 0.042 - ETA: 5s - loss: 0.040 - ETA: 5s - loss: 0.038 - ETA: 5s - loss: 0.039 - ETA: 5s - loss: 0.040 - ETA: 5s - loss: 0.039 - ETA: 4s - loss: 0.039 - ETA: 4s - loss: 0.038 - ETA: 4s - loss: 0.037 - ETA: 4s - loss: 0.037 - ETA: 4s - loss: 0.037 - ETA: 4s - loss: 0.037 - ETA: 3s - loss: 0.037 - ETA: 3s - loss: 0.037 - ETA: 3s - loss: 0.037 - ETA: 3s - loss: 0.039 - ETA: 3s - loss: 0.039 - ETA: 3s - loss: 0.040 - ETA: 2s - loss: 0.040 - ETA: 2s - loss: 0.040 - ETA: 2s - loss: 0.041 - ETA: 2s - loss: 0.041 - ETA: 2s - loss: 0.042 - ETA: 2s - loss: 0.042 - ETA: 1s - loss: 0.043 - ETA: 1s - loss: 0.043 - ETA: 1s - loss: 0.043 - ETA: 1s - loss: 0.043 - ETA: 1s - loss: 0.044 - ETA: 1s - loss: 0.044 - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.044 - 7s 340us/step - loss: 0.0450\n",
      "\n",
      "Epoch 00008: loss improved from 0.04944 to 0.04498, saving model to models\\weights-improvement-08-bigger.hdf5\n",
      "Epoch 9/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.036 - ETA: 6s - loss: 0.034 - ETA: 6s - loss: 0.032 - ETA: 6s - loss: 0.034 - ETA: 5s - loss: 0.036 - ETA: 5s - loss: 0.035 - ETA: 5s - loss: 0.036 - ETA: 5s - loss: 0.036 - ETA: 5s - loss: 0.037 - ETA: 4s - loss: 0.038 - ETA: 4s - loss: 0.037 - ETA: 4s - loss: 0.037 - ETA: 4s - loss: 0.038 - ETA: 4s - loss: 0.037 - ETA: 4s - loss: 0.037 - ETA: 3s - loss: 0.037 - ETA: 3s - loss: 0.037 - ETA: 3s - loss: 0.037 - ETA: 3s - loss: 0.036 - ETA: 3s - loss: 0.037 - ETA: 3s - loss: 0.036 - ETA: 2s - loss: 0.036 - ETA: 2s - loss: 0.036 - ETA: 2s - loss: 0.036 - ETA: 2s - loss: 0.037 - ETA: 2s - loss: 0.037 - ETA: 2s - loss: 0.036 - ETA: 1s - loss: 0.036 - ETA: 1s - loss: 0.037 - ETA: 1s - loss: 0.037 - ETA: 1s - loss: 0.037 - ETA: 1s - loss: 0.037 - ETA: 1s - loss: 0.037 - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.037 - 7s 340us/step - loss: 0.0373\n",
      "\n",
      "Epoch 00009: loss improved from 0.04498 to 0.03727, saving model to models\\weights-improvement-09-bigger.hdf5\n",
      "Epoch 10/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.032 - ETA: 6s - loss: 0.027 - ETA: 6s - loss: 0.027 - ETA: 6s - loss: 0.030 - ETA: 5s - loss: 0.033 - ETA: 5s - loss: 0.033 - ETA: 5s - loss: 0.034 - ETA: 5s - loss: 0.033 - ETA: 5s - loss: 0.034 - ETA: 5s - loss: 0.034 - ETA: 4s - loss: 0.033 - ETA: 4s - loss: 0.033 - ETA: 4s - loss: 0.034 - ETA: 4s - loss: 0.034 - ETA: 4s - loss: 0.033 - ETA: 3s - loss: 0.033 - ETA: 3s - loss: 0.034 - ETA: 3s - loss: 0.033 - ETA: 3s - loss: 0.033 - ETA: 3s - loss: 0.032 - ETA: 3s - loss: 0.032 - ETA: 2s - loss: 0.032 - ETA: 2s - loss: 0.032 - ETA: 2s - loss: 0.032 - ETA: 2s - loss: 0.032 - ETA: 2s - loss: 0.031 - ETA: 2s - loss: 0.032 - ETA: 1s - loss: 0.032 - ETA: 1s - loss: 0.031 - ETA: 1s - loss: 0.032 - ETA: 1s - loss: 0.032 - ETA: 1s - loss: 0.032 - ETA: 1s - loss: 0.032 - ETA: 0s - loss: 0.032 - ETA: 0s - loss: 0.032 - ETA: 0s - loss: 0.032 - ETA: 0s - loss: 0.032 - ETA: 0s - loss: 0.032 - 7s 341us/step - loss: 0.0323\n",
      "\n",
      "Epoch 00010: loss improved from 0.03727 to 0.03233, saving model to models\\weights-improvement-10-bigger.hdf5\n",
      "Epoch 11/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.021 - ETA: 6s - loss: 0.028 - ETA: 6s - loss: 0.025 - ETA: 6s - loss: 0.028 - ETA: 5s - loss: 0.028 - ETA: 5s - loss: 0.028 - ETA: 5s - loss: 0.027 - ETA: 5s - loss: 0.027 - ETA: 5s - loss: 0.027 - ETA: 5s - loss: 0.027 - ETA: 4s - loss: 0.027 - ETA: 4s - loss: 0.027 - ETA: 4s - loss: 0.027 - ETA: 4s - loss: 0.027 - ETA: 4s - loss: 0.027 - ETA: 3s - loss: 0.027 - ETA: 3s - loss: 0.026 - ETA: 3s - loss: 0.026 - ETA: 3s - loss: 0.026 - ETA: 3s - loss: 0.027 - ETA: 3s - loss: 0.027 - ETA: 2s - loss: 0.028 - ETA: 2s - loss: 0.028 - ETA: 2s - loss: 0.028 - ETA: 2s - loss: 0.028 - ETA: 2s - loss: 0.029 - ETA: 2s - loss: 0.028 - ETA: 1s - loss: 0.028 - ETA: 1s - loss: 0.029 - ETA: 1s - loss: 0.029 - ETA: 1s - loss: 0.029 - ETA: 1s - loss: 0.029 - ETA: 1s - loss: 0.029 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.029 - 7s 339us/step - loss: 0.0298\n",
      "\n",
      "Epoch 00011: loss improved from 0.03233 to 0.02983, saving model to models\\weights-improvement-11-bigger.hdf5\n",
      "Epoch 12/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.024 - ETA: 6s - loss: 0.023 - ETA: 6s - loss: 0.022 - ETA: 5s - loss: 0.023 - ETA: 5s - loss: 0.021 - ETA: 5s - loss: 0.021 - ETA: 5s - loss: 0.021 - ETA: 5s - loss: 0.021 - ETA: 5s - loss: 0.021 - ETA: 4s - loss: 0.021 - ETA: 4s - loss: 0.021 - ETA: 4s - loss: 0.021 - ETA: 4s - loss: 0.020 - ETA: 4s - loss: 0.021 - ETA: 4s - loss: 0.020 - ETA: 3s - loss: 0.020 - ETA: 3s - loss: 0.021 - ETA: 3s - loss: 0.021 - ETA: 3s - loss: 0.021 - ETA: 3s - loss: 0.021 - ETA: 3s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 1s - loss: 0.022 - ETA: 1s - loss: 0.022 - ETA: 1s - loss: 0.023 - ETA: 1s - loss: 0.022 - ETA: 1s - loss: 0.022 - ETA: 1s - loss: 0.022 - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.022 - 7s 340us/step - loss: 0.0229\n",
      "\n",
      "Epoch 00012: loss improved from 0.02983 to 0.02289, saving model to models\\weights-improvement-12-bigger.hdf5\n",
      "Epoch 13/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.020 - ETA: 6s - loss: 0.019 - ETA: 6s - loss: 0.018 - ETA: 6s - loss: 0.018 - ETA: 5s - loss: 0.019 - ETA: 5s - loss: 0.019 - ETA: 5s - loss: 0.019 - ETA: 5s - loss: 0.020 - ETA: 5s - loss: 0.020 - ETA: 5s - loss: 0.020 - ETA: 4s - loss: 0.020 - ETA: 4s - loss: 0.019 - ETA: 4s - loss: 0.019 - ETA: 4s - loss: 0.019 - ETA: 4s - loss: 0.019 - ETA: 3s - loss: 0.019 - ETA: 3s - loss: 0.019 - ETA: 3s - loss: 0.020 - ETA: 3s - loss: 0.020 - ETA: 3s - loss: 0.020 - ETA: 3s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.020 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.021 - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.021 - 7s 339us/step - loss: 0.0213\n",
      "\n",
      "Epoch 00013: loss improved from 0.02289 to 0.02126, saving model to models\\weights-improvement-13-bigger.hdf5\n",
      "Epoch 14/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.009 - ETA: 6s - loss: 0.012 - ETA: 6s - loss: 0.014 - ETA: 6s - loss: 0.015 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.016 - ETA: 5s - loss: 0.016 - ETA: 5s - loss: 0.018 - ETA: 5s - loss: 0.018 - ETA: 5s - loss: 0.018 - ETA: 4s - loss: 0.017 - ETA: 4s - loss: 0.017 - ETA: 4s - loss: 0.018 - ETA: 4s - loss: 0.018 - ETA: 4s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - 7s 340us/step - loss: 0.0202\n",
      "\n",
      "Epoch 00014: loss improved from 0.02126 to 0.02017, saving model to models\\weights-improvement-14-bigger.hdf5\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19910/19910 [==============================] - ETA: 6s - loss: 0.014 - ETA: 6s - loss: 0.015 - ETA: 6s - loss: 0.014 - ETA: 6s - loss: 0.014 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.016 - ETA: 5s - loss: 0.016 - ETA: 5s - loss: 0.016 - ETA: 5s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - 7s 340us/step - loss: 0.0185\n",
      "\n",
      "Epoch 00015: loss improved from 0.02017 to 0.01849, saving model to models\\weights-improvement-15-bigger.hdf5\n",
      "Epoch 16/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.009 - ETA: 6s - loss: 0.011 - ETA: 6s - loss: 0.015 - ETA: 6s - loss: 0.014 - ETA: 5s - loss: 0.016 - ETA: 5s - loss: 0.015 - ETA: 5s - loss: 0.016 - ETA: 5s - loss: 0.016 - ETA: 5s - loss: 0.017 - ETA: 4s - loss: 0.017 - ETA: 4s - loss: 0.017 - ETA: 4s - loss: 0.018 - ETA: 4s - loss: 0.018 - ETA: 4s - loss: 0.018 - ETA: 4s - loss: 0.018 - ETA: 3s - loss: 0.019 - ETA: 3s - loss: 0.019 - ETA: 3s - loss: 0.019 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.019 - ETA: 3s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - 7s 339us/step - loss: 0.0196\n",
      "\n",
      "Epoch 00016: loss did not improve from 0.01849\n",
      "Epoch 17/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.021 - ETA: 6s - loss: 0.015 - ETA: 6s - loss: 0.017 - ETA: 5s - loss: 0.015 - ETA: 5s - loss: 0.015 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.015 - ETA: 5s - loss: 0.015 - ETA: 5s - loss: 0.016 - ETA: 4s - loss: 0.017 - ETA: 4s - loss: 0.017 - ETA: 4s - loss: 0.017 - ETA: 4s - loss: 0.017 - ETA: 4s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - 7s 339us/step - loss: 0.0178\n",
      "\n",
      "Epoch 00017: loss improved from 0.01849 to 0.01781, saving model to models\\weights-improvement-17-bigger.hdf5\n",
      "Epoch 18/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.026 - ETA: 6s - loss: 0.017 - ETA: 6s - loss: 0.015 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.013 - ETA: 5s - loss: 0.013 - ETA: 5s - loss: 0.012 - ETA: 5s - loss: 0.012 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 7s 353us/step - loss: 0.0128\n",
      "\n",
      "Epoch 00018: loss improved from 0.01781 to 0.01281, saving model to models\\weights-improvement-18-bigger.hdf5\n",
      "Epoch 19/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.017 - ETA: 6s - loss: 0.018 - ETA: 5s - loss: 0.016 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.013 - ETA: 5s - loss: 0.013 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.013 - ETA: 5s - loss: 0.012 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - 7s 339us/step - loss: 0.0143\n",
      "\n",
      "Epoch 00019: loss did not improve from 0.01281\n",
      "Epoch 20/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.010 - ETA: 6s - loss: 0.017 - ETA: 6s - loss: 0.018 - ETA: 6s - loss: 0.018 - ETA: 5s - loss: 0.017 - ETA: 5s - loss: 0.016 - ETA: 5s - loss: 0.015 - ETA: 5s - loss: 0.015 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 7s 347us/step - loss: 0.0121\n",
      "\n",
      "Epoch 00020: loss improved from 0.01281 to 0.01214, saving model to models\\weights-improvement-20-bigger.hdf5\n",
      "Epoch 21/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.015 - ETA: 6s - loss: 0.017 - ETA: 6s - loss: 0.014 - ETA: 5s - loss: 0.013 - ETA: 5s - loss: 0.013 - ETA: 5s - loss: 0.011 - ETA: 5s - loss: 0.012 - ETA: 5s - loss: 0.011 - ETA: 5s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.013 - 7s 361us/step - loss: 0.0137\n",
      "\n",
      "Epoch 00021: loss did not improve from 0.01214\n",
      "Epoch 22/50\n",
      "19910/19910 [==============================] - ETA: 7s - loss: 0.009 - ETA: 7s - loss: 0.010 - ETA: 6s - loss: 0.009 - ETA: 6s - loss: 0.010 - ETA: 6s - loss: 0.011 - ETA: 6s - loss: 0.010 - ETA: 5s - loss: 0.010 - ETA: 5s - loss: 0.010 - ETA: 5s - loss: 0.010 - ETA: 5s - loss: 0.010 - ETA: 5s - loss: 0.010 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 7s 344us/step - loss: 0.0099\n",
      "\n",
      "Epoch 00022: loss improved from 0.01214 to 0.00989, saving model to models\\weights-improvement-22-bigger.hdf5\n",
      "Epoch 23/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.004 - ETA: 6s - loss: 0.007 - ETA: 6s - loss: 0.006 - ETA: 5s - loss: 0.006 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.006 - ETA: 5s - loss: 0.006 - ETA: 5s - loss: 0.006 - ETA: 4s - loss: 0.006 - ETA: 4s - loss: 0.005 - ETA: 4s - loss: 0.005 - ETA: 4s - loss: 0.006 - ETA: 4s - loss: 0.005 - ETA: 4s - loss: 0.006 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.006 - ETA: 2s - loss: 0.006 - ETA: 2s - loss: 0.006 - ETA: 2s - loss: 0.006 - ETA: 2s - loss: 0.006 - ETA: 2s - loss: 0.006 - ETA: 2s - loss: 0.006 - ETA: 1s - loss: 0.006 - ETA: 1s - loss: 0.006 - ETA: 1s - loss: 0.006 - ETA: 1s - loss: 0.006 - ETA: 1s - loss: 0.006 - ETA: 1s - loss: 0.006 - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - 7s 338us/step - loss: 0.0071\n",
      "\n",
      "Epoch 00023: loss improved from 0.00989 to 0.00713, saving model to models\\weights-improvement-23-bigger.hdf5\n",
      "Epoch 24/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.004 - ETA: 6s - loss: 0.004 - ETA: 6s - loss: 0.004 - ETA: 5s - loss: 0.003 - ETA: 5s - loss: 0.003 - ETA: 5s - loss: 0.005 - ETA: 5s - loss: 0.005 - ETA: 5s - loss: 0.005 - ETA: 5s - loss: 0.005 - ETA: 4s - loss: 0.005 - ETA: 4s - loss: 0.005 - ETA: 4s - loss: 0.005 - ETA: 4s - loss: 0.005 - ETA: 4s - loss: 0.005 - ETA: 4s - loss: 0.005 - ETA: 3s - loss: 0.005 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.006 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 7s 336us/step - loss: 0.0086\n",
      "\n",
      "Epoch 00024: loss did not improve from 0.00713\n",
      "Epoch 25/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.011 - ETA: 6s - loss: 0.009 - ETA: 6s - loss: 0.008 - ETA: 5s - loss: 0.010 - ETA: 5s - loss: 0.009 - ETA: 5s - loss: 0.009 - ETA: 5s - loss: 0.009 - ETA: 5s - loss: 0.009 - ETA: 5s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 7s 336us/step - loss: 0.0104\n",
      "\n",
      "Epoch 00025: loss did not improve from 0.00713\n",
      "Epoch 26/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.003 - ETA: 6s - loss: 0.004 - ETA: 6s - loss: 0.006 - ETA: 5s - loss: 0.005 - ETA: 5s - loss: 0.006 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 7s 349us/step - loss: 0.0092\n",
      "\n",
      "Epoch 00026: loss did not improve from 0.00713\n",
      "Epoch 27/50\n",
      "19910/19910 [==============================] - ETA: 7s - loss: 0.002 - ETA: 6s - loss: 0.006 - ETA: 6s - loss: 0.009 - ETA: 6s - loss: 0.010 - ETA: 5s - loss: 0.009 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 7s 334us/step - loss: 0.0087\n",
      "\n",
      "Epoch 00027: loss did not improve from 0.00713\n",
      "Epoch 28/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.009 - ETA: 6s - loss: 0.009 - ETA: 6s - loss: 0.007 - ETA: 6s - loss: 0.007 - ETA: 6s - loss: 0.007 - ETA: 6s - loss: 0.006 - ETA: 5s - loss: 0.006 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 7s 342us/step - loss: 0.0086\n",
      "\n",
      "Epoch 00028: loss did not improve from 0.00713\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19910/19910 [==============================] - ETA: 6s - loss: 0.006 - ETA: 6s - loss: 0.008 - ETA: 6s - loss: 0.006 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.006 - ETA: 2s - loss: 0.006 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - 7s 334us/step - loss: 0.0075\n",
      "\n",
      "Epoch 00029: loss did not improve from 0.00713\n",
      "Epoch 30/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.004 - ETA: 6s - loss: 0.004 - ETA: 6s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 4s - loss: 0.003 - ETA: 4s - loss: 0.003 - ETA: 4s - loss: 0.003 - ETA: 3s - loss: 0.003 - ETA: 3s - loss: 0.003 - ETA: 3s - loss: 0.003 - ETA: 3s - loss: 0.003 - ETA: 3s - loss: 0.003 - ETA: 3s - loss: 0.003 - ETA: 2s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 1s - loss: 0.004 - ETA: 1s - loss: 0.004 - ETA: 1s - loss: 0.004 - ETA: 1s - loss: 0.004 - ETA: 1s - loss: 0.004 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - 7s 332us/step - loss: 0.0048\n",
      "\n",
      "Epoch 00030: loss improved from 0.00713 to 0.00480, saving model to models\\weights-improvement-30-bigger.hdf5\n",
      "Epoch 31/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.008 - ETA: 6s - loss: 0.006 - ETA: 6s - loss: 0.007 - ETA: 5s - loss: 0.006 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.009 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 7s 331us/step - loss: 0.0112\n",
      "\n",
      "Epoch 00031: loss did not improve from 0.00480\n",
      "Epoch 32/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.006 - ETA: 6s - loss: 0.005 - ETA: 6s - loss: 0.006 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - 7s 331us/step - loss: 0.0077\n",
      "\n",
      "Epoch 00032: loss did not improve from 0.00480\n",
      "Epoch 33/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.005 - ETA: 6s - loss: 0.005 - ETA: 6s - loss: 0.006 - ETA: 5s - loss: 0.006 - ETA: 5s - loss: 0.005 - ETA: 5s - loss: 0.006 - ETA: 5s - loss: 0.006 - ETA: 5s - loss: 0.005 - ETA: 5s - loss: 0.006 - ETA: 4s - loss: 0.006 - ETA: 4s - loss: 0.006 - ETA: 4s - loss: 0.006 - ETA: 4s - loss: 0.006 - ETA: 4s - loss: 0.006 - ETA: 4s - loss: 0.006 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.006 - ETA: 2s - loss: 0.005 - ETA: 2s - loss: 0.005 - ETA: 2s - loss: 0.005 - ETA: 2s - loss: 0.005 - ETA: 2s - loss: 0.005 - ETA: 2s - loss: 0.005 - ETA: 1s - loss: 0.005 - ETA: 1s - loss: 0.005 - ETA: 1s - loss: 0.005 - ETA: 1s - loss: 0.005 - ETA: 1s - loss: 0.005 - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - 7s 331us/step - loss: 0.0054\n",
      "\n",
      "Epoch 00033: loss did not improve from 0.00480\n",
      "Epoch 34/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.004 - ETA: 6s - loss: 0.005 - ETA: 6s - loss: 0.004 - ETA: 5s - loss: 0.005 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 4s - loss: 0.005 - ETA: 4s - loss: 0.005 - ETA: 4s - loss: 0.005 - ETA: 4s - loss: 0.005 - ETA: 4s - loss: 0.005 - ETA: 3s - loss: 0.005 - ETA: 3s - loss: 0.005 - ETA: 3s - loss: 0.005 - ETA: 3s - loss: 0.005 - ETA: 3s - loss: 0.005 - ETA: 3s - loss: 0.005 - ETA: 2s - loss: 0.005 - ETA: 2s - loss: 0.005 - ETA: 2s - loss: 0.005 - ETA: 2s - loss: 0.005 - ETA: 2s - loss: 0.005 - ETA: 1s - loss: 0.005 - ETA: 1s - loss: 0.005 - ETA: 1s - loss: 0.005 - ETA: 1s - loss: 0.005 - ETA: 1s - loss: 0.005 - ETA: 1s - loss: 0.005 - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 7s 348us/step - loss: 0.0062\n",
      "\n",
      "Epoch 00034: loss did not improve from 0.00480\n",
      "Epoch 35/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.005 - ETA: 6s - loss: 0.006 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.006 - ETA: 5s - loss: 0.006 - ETA: 5s - loss: 0.006 - ETA: 5s - loss: 0.006 - ETA: 5s - loss: 0.006 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - 7s 332us/step - loss: 0.0074\n",
      "\n",
      "Epoch 00035: loss did not improve from 0.00480\n",
      "Epoch 36/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.003 - ETA: 6s - loss: 0.003 - ETA: 6s - loss: 0.004 - ETA: 5s - loss: 0.003 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 4s - loss: 0.005 - ETA: 4s - loss: 0.005 - ETA: 4s - loss: 0.005 - ETA: 4s - loss: 0.005 - ETA: 4s - loss: 0.005 - ETA: 3s - loss: 0.005 - ETA: 3s - loss: 0.005 - ETA: 3s - loss: 0.005 - ETA: 3s - loss: 0.005 - ETA: 3s - loss: 0.005 - ETA: 3s - loss: 0.005 - ETA: 2s - loss: 0.005 - ETA: 2s - loss: 0.005 - ETA: 2s - loss: 0.005 - ETA: 2s - loss: 0.005 - ETA: 2s - loss: 0.005 - ETA: 2s - loss: 0.005 - ETA: 1s - loss: 0.005 - ETA: 1s - loss: 0.005 - ETA: 1s - loss: 0.005 - ETA: 1s - loss: 0.005 - ETA: 1s - loss: 0.005 - ETA: 1s - loss: 0.006 - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 7s 332us/step - loss: 0.0061\n",
      "\n",
      "Epoch 00036: loss did not improve from 0.00480\n",
      "Epoch 37/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 8.3998e-0 - ETA: 6s - loss: 0.0028    - ETA: 6s - loss: 0.006 - ETA: 6s - loss: 0.006 - ETA: 5s - loss: 0.006 - ETA: 5s - loss: 0.005 - ETA: 5s - loss: 0.006 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.006 - ETA: 5s - loss: 0.006 - ETA: 4s - loss: 0.006 - ETA: 4s - loss: 0.006 - ETA: 4s - loss: 0.006 - ETA: 4s - loss: 0.006 - ETA: 4s - loss: 0.006 - ETA: 4s - loss: 0.006 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.006 - ETA: 2s - loss: 0.006 - ETA: 2s - loss: 0.006 - ETA: 2s - loss: 0.006 - ETA: 2s - loss: 0.006 - ETA: 2s - loss: 0.005 - ETA: 2s - loss: 0.005 - ETA: 1s - loss: 0.005 - ETA: 1s - loss: 0.005 - ETA: 1s - loss: 0.005 - ETA: 1s - loss: 0.005 - ETA: 1s - loss: 0.005 - ETA: 1s - loss: 0.005 - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - 7s 339us/step - loss: 0.0061\n",
      "\n",
      "Epoch 00037: loss did not improve from 0.00480\n",
      "Epoch 38/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.003 - ETA: 6s - loss: 0.003 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.003 - ETA: 5s - loss: 0.006 - ETA: 5s - loss: 0.006 - ETA: 4s - loss: 0.006 - ETA: 4s - loss: 0.006 - ETA: 4s - loss: 0.005 - ETA: 4s - loss: 0.005 - ETA: 4s - loss: 0.005 - ETA: 4s - loss: 0.005 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.005 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.005 - ETA: 3s - loss: 0.005 - ETA: 2s - loss: 0.006 - ETA: 2s - loss: 0.006 - ETA: 2s - loss: 0.006 - ETA: 2s - loss: 0.006 - ETA: 2s - loss: 0.006 - ETA: 2s - loss: 0.006 - ETA: 1s - loss: 0.006 - ETA: 1s - loss: 0.006 - ETA: 1s - loss: 0.006 - ETA: 1s - loss: 0.006 - ETA: 1s - loss: 0.006 - ETA: 1s - loss: 0.006 - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 7s 328us/step - loss: 0.0063\n",
      "\n",
      "Epoch 00038: loss did not improve from 0.00480\n",
      "Epoch 39/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.006 - ETA: 6s - loss: 0.005 - ETA: 6s - loss: 0.004 - ETA: 6s - loss: 0.005 - ETA: 6s - loss: 0.006 - ETA: 6s - loss: 0.006 - ETA: 6s - loss: 0.006 - ETA: 5s - loss: 0.006 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.006 - ETA: 5s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.006 - ETA: 4s - loss: 0.006 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 8s 408us/step - loss: 0.0067\n",
      "\n",
      "Epoch 00039: loss did not improve from 0.00480\n",
      "Epoch 40/50\n",
      "19910/19910 [==============================] - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.003 - ETA: 6s - loss: 0.005 - ETA: 7s - loss: 0.004 - ETA: 6s - loss: 0.004 - ETA: 6s - loss: 0.004 - ETA: 6s - loss: 0.004 - ETA: 6s - loss: 0.004 - ETA: 6s - loss: 0.003 - ETA: 6s - loss: 0.003 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.005 - ETA: 5s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 3s - loss: 0.004 - ETA: 3s - loss: 0.004 - ETA: 3s - loss: 0.004 - ETA: 3s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 1s - loss: 0.004 - ETA: 1s - loss: 0.004 - ETA: 1s - loss: 0.004 - ETA: 1s - loss: 0.004 - ETA: 1s - loss: 0.004 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - 8s 421us/step - loss: 0.0043\n",
      "\n",
      "Epoch 00040: loss improved from 0.00480 to 0.00434, saving model to models\\weights-improvement-40-bigger.hdf5\n",
      "Epoch 41/50\n",
      "19910/19910 [==============================] - ETA: 7s - loss: 0.003 - ETA: 7s - loss: 0.004 - ETA: 6s - loss: 0.005 - ETA: 6s - loss: 0.006 - ETA: 6s - loss: 0.006 - ETA: 6s - loss: 0.005 - ETA: 6s - loss: 0.005 - ETA: 6s - loss: 0.004 - ETA: 5s - loss: 0.005 - ETA: 5s - loss: 0.005 - ETA: 5s - loss: 0.005 - ETA: 5s - loss: 0.005 - ETA: 5s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 3s - loss: 0.004 - ETA: 3s - loss: 0.004 - ETA: 3s - loss: 0.004 - ETA: 3s - loss: 0.004 - ETA: 3s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 2s - loss: 0.003 - ETA: 2s - loss: 0.003 - ETA: 1s - loss: 0.003 - ETA: 1s - loss: 0.003 - ETA: 1s - loss: 0.003 - ETA: 1s - loss: 0.003 - ETA: 1s - loss: 0.003 - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.003 - 8s 397us/step - loss: 0.0037\n",
      "\n",
      "Epoch 00041: loss improved from 0.00434 to 0.00370, saving model to models\\weights-improvement-41-bigger.hdf5\n",
      "Epoch 42/50\n",
      "19910/19910 [==============================] - ETA: 8s - loss: 0.002 - ETA: 7s - loss: 0.002 - ETA: 7s - loss: 0.002 - ETA: 7s - loss: 0.003 - ETA: 6s - loss: 0.003 - ETA: 6s - loss: 0.003 - ETA: 6s - loss: 0.004 - ETA: 6s - loss: 0.004 - ETA: 6s - loss: 0.004 - ETA: 6s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.005 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 3s - loss: 0.004 - ETA: 3s - loss: 0.004 - ETA: 3s - loss: 0.004 - ETA: 3s - loss: 0.004 - ETA: 3s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 1s - loss: 0.004 - ETA: 1s - loss: 0.004 - ETA: 1s - loss: 0.004 - ETA: 1s - loss: 0.004 - ETA: 1s - loss: 0.004 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - 8s 405us/step - loss: 0.0047\n",
      "\n",
      "Epoch 00042: loss did not improve from 0.00370\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19910/19910 [==============================] - ETA: 7s - loss: 0.006 - ETA: 7s - loss: 0.006 - ETA: 7s - loss: 0.004 - ETA: 7s - loss: 0.004 - ETA: 6s - loss: 0.004 - ETA: 6s - loss: 0.004 - ETA: 6s - loss: 0.004 - ETA: 6s - loss: 0.005 - ETA: 5s - loss: 0.005 - ETA: 5s - loss: 0.005 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 4s - loss: 0.005 - ETA: 4s - loss: 0.005 - ETA: 4s - loss: 0.004 - ETA: 4s - loss: 0.005 - ETA: 4s - loss: 0.005 - ETA: 3s - loss: 0.005 - ETA: 3s - loss: 0.005 - ETA: 3s - loss: 0.005 - ETA: 3s - loss: 0.004 - ETA: 3s - loss: 0.004 - ETA: 2s - loss: 0.005 - ETA: 2s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 1s - loss: 0.004 - ETA: 1s - loss: 0.004 - ETA: 1s - loss: 0.004 - ETA: 1s - loss: 0.004 - ETA: 1s - loss: 0.004 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - 8s 392us/step - loss: 0.0045\n",
      "\n",
      "Epoch 00043: loss did not improve from 0.00370\n",
      "Epoch 44/50\n",
      "19910/19910 [==============================] - ETA: 7s - loss: 0.003 - ETA: 7s - loss: 0.004 - ETA: 7s - loss: 0.003 - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 5s - loss: 0.002 - ETA: 5s - loss: 0.002 - ETA: 5s - loss: 0.003 - ETA: 5s - loss: 0.003 - ETA: 5s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 4s - loss: 0.003 - ETA: 4s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 3s - loss: 0.002 - ETA: 3s - loss: 0.002 - ETA: 3s - loss: 0.003 - ETA: 3s - loss: 0.003 - ETA: 3s - loss: 0.003 - ETA: 2s - loss: 0.003 - ETA: 2s - loss: 0.003 - ETA: 2s - loss: 0.003 - ETA: 2s - loss: 0.003 - ETA: 2s - loss: 0.003 - ETA: 1s - loss: 0.003 - ETA: 1s - loss: 0.003 - ETA: 1s - loss: 0.003 - ETA: 1s - loss: 0.003 - ETA: 1s - loss: 0.003 - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - 8s 392us/step - loss: 0.0046\n",
      "\n",
      "Epoch 00044: loss did not improve from 0.00370\n",
      "Epoch 45/50\n",
      "19910/19910 [==============================] - ETA: 7s - loss: 0.001 - ETA: 7s - loss: 0.002 - ETA: 7s - loss: 0.003 - ETA: 7s - loss: 0.008 - ETA: 6s - loss: 0.008 - ETA: 6s - loss: 0.008 - ETA: 6s - loss: 0.007 - ETA: 6s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - 8s 389us/step - loss: 0.0076\n",
      "\n",
      "Epoch 00045: loss did not improve from 0.00370\n",
      "Epoch 46/50\n",
      "19910/19910 [==============================] - ETA: 7s - loss: 0.001 - ETA: 7s - loss: 0.001 - ETA: 7s - loss: 0.001 - ETA: 6s - loss: 0.001 - ETA: 6s - loss: 0.003 - ETA: 6s - loss: 0.003 - ETA: 6s - loss: 0.003 - ETA: 6s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 3s - loss: 0.004 - ETA: 3s - loss: 0.004 - ETA: 3s - loss: 0.004 - ETA: 3s - loss: 0.004 - ETA: 3s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 2s - loss: 0.005 - ETA: 2s - loss: 0.005 - ETA: 2s - loss: 0.005 - ETA: 2s - loss: 0.004 - ETA: 1s - loss: 0.004 - ETA: 1s - loss: 0.005 - ETA: 1s - loss: 0.005 - ETA: 1s - loss: 0.005 - ETA: 1s - loss: 0.005 - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - 8s 386us/step - loss: 0.0056\n",
      "\n",
      "Epoch 00046: loss did not improve from 0.00370\n",
      "Epoch 47/50\n",
      "19910/19910 [==============================] - ETA: 7s - loss: 0.003 - ETA: 7s - loss: 0.003 - ETA: 7s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.003 - ETA: 6s - loss: 0.003 - ETA: 6s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.003 - ETA: 5s - loss: 0.003 - ETA: 5s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 4s - loss: 0.003 - ETA: 4s - loss: 0.003 - ETA: 4s - loss: 0.003 - ETA: 4s - loss: 0.003 - ETA: 3s - loss: 0.003 - ETA: 3s - loss: 0.003 - ETA: 3s - loss: 0.003 - ETA: 3s - loss: 0.003 - ETA: 3s - loss: 0.003 - ETA: 2s - loss: 0.003 - ETA: 2s - loss: 0.003 - ETA: 2s - loss: 0.003 - ETA: 2s - loss: 0.003 - ETA: 2s - loss: 0.003 - ETA: 1s - loss: 0.003 - ETA: 1s - loss: 0.003 - ETA: 1s - loss: 0.003 - ETA: 1s - loss: 0.003 - ETA: 1s - loss: 0.003 - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.003 - 8s 391us/step - loss: 0.0037\n",
      "\n",
      "Epoch 00047: loss did not improve from 0.00370\n",
      "Epoch 48/50\n",
      "19910/19910 [==============================] - ETA: 7s - loss: 0.001 - ETA: 7s - loss: 0.001 - ETA: 7s - loss: 0.003 - ETA: 7s - loss: 0.005 - ETA: 7s - loss: 0.005 - ETA: 6s - loss: 0.005 - ETA: 6s - loss: 0.004 - ETA: 6s - loss: 0.005 - ETA: 6s - loss: 0.005 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 5s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 4s - loss: 0.004 - ETA: 3s - loss: 0.004 - ETA: 3s - loss: 0.004 - ETA: 3s - loss: 0.004 - ETA: 3s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 1s - loss: 0.004 - ETA: 1s - loss: 0.004 - ETA: 1s - loss: 0.004 - ETA: 1s - loss: 0.004 - ETA: 1s - loss: 0.004 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - 8s 391us/step - loss: 0.0044\n",
      "\n",
      "Epoch 00048: loss did not improve from 0.00370\n",
      "Epoch 49/50\n",
      "19910/19910 [==============================] - ETA: 7s - loss: 0.001 - ETA: 7s - loss: 0.001 - ETA: 6s - loss: 0.001 - ETA: 7s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 6s - loss: 0.002 - ETA: 5s - loss: 0.002 - ETA: 5s - loss: 0.002 - ETA: 5s - loss: 0.002 - ETA: 5s - loss: 0.002 - ETA: 5s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 4s - loss: 0.003 - ETA: 4s - loss: 0.003 - ETA: 3s - loss: 0.002 - ETA: 3s - loss: 0.003 - ETA: 3s - loss: 0.003 - ETA: 3s - loss: 0.003 - ETA: 3s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 2s - loss: 0.004 - ETA: 1s - loss: 0.004 - ETA: 1s - loss: 0.004 - ETA: 1s - loss: 0.003 - ETA: 1s - loss: 0.003 - ETA: 1s - loss: 0.004 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - 8s 388us/step - loss: 0.0045\n",
      "\n",
      "Epoch 00049: loss did not improve from 0.00370\n",
      "Epoch 50/50\n",
      "19910/19910 [==============================] - ETA: 7s - loss: 0.004 - ETA: 7s - loss: 0.004 - ETA: 6s - loss: 0.004 - ETA: 6s - loss: 0.004 - ETA: 6s - loss: 0.004 - ETA: 6s - loss: 0.004 - ETA: 6s - loss: 0.003 - ETA: 6s - loss: 0.003 - ETA: 5s - loss: 0.003 - ETA: 5s - loss: 0.003 - ETA: 5s - loss: 0.002 - ETA: 5s - loss: 0.002 - ETA: 5s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 4s - loss: 0.002 - ETA: 3s - loss: 0.002 - ETA: 3s - loss: 0.002 - ETA: 3s - loss: 0.002 - ETA: 3s - loss: 0.002 - ETA: 3s - loss: 0.002 - ETA: 2s - loss: 0.002 - ETA: 2s - loss: 0.002 - ETA: 2s - loss: 0.002 - ETA: 2s - loss: 0.002 - ETA: 2s - loss: 0.002 - ETA: 2s - loss: 0.002 - ETA: 1s - loss: 0.002 - ETA: 1s - loss: 0.002 - ETA: 1s - loss: 0.002 - ETA: 1s - loss: 0.002 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - 8s 398us/step - loss: 0.0026\n",
      "\n",
      "Epoch 00050: loss improved from 0.00370 to 0.00256, saving model to models\\weights-improvement-50-bigger.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29480002400>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "test_patients = []\n",
    "train_patients = []\n",
    "test_size = 0.2\n",
    "channels\n",
    "for label in selected_labels:\n",
    "    df_selected = df_records.loc[df_records['label'] == label]\n",
    "    patients = df_selected['patient'].unique()\n",
    "    n_test = math.ceil(len(patients)*test_size)\n",
    "    test_patients+=list(np.random.choice(patients, n_test, replace=False))\n",
    "    train_patients+=list(patients[np.isin(patients, test_patients, invert=True)])\n",
    "    \n",
    "df_patient_records = df_records.set_index('patient')\n",
    "df_train_patients = df_patient_records.loc[train_patients]\n",
    "df_test_patients = df_patient_records.loc[test_patients]\n",
    "window_size = 2048#df_records['signal_length'].min()\n",
    "trainX, trainY, _ = make_set(df_train_patients, channels, label_map, False, window_size)\n",
    "testX, testY, record_list = make_set(df_test_patients, channels, label_map, True, window_size)\n",
    "\n",
    "#Shuffle order of train set\n",
    "trainX, trainY = shuffle(trainX, trainY)\n",
    "\n",
    "#Since we have a large class inbalance we need to udjust the weights for it.\n",
    "fractions = 1-trainY.sum(axis=0)/len(trainY)\n",
    "weights = fractions[trainY.argmax(axis=1)]\n",
    "\n",
    "#df_selected['patient'].sample(len())\n",
    "\n",
    "filepath = os.path.join('models', \"weights-improvement-{epoch:02d}-bigger.hdf5\")\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "model_name = 'two_classes'\n",
    "model_folder = os.path.join('tensorlogs', model_name + \"-logs/\")\n",
    "\n",
    "if not os.path.isdir(model_folder):\n",
    "    n_logs = 0\n",
    "else:\n",
    "    n_logs = len(os.listdir(model_folder))\n",
    "    \n",
    "tensorboard_logs = os.path.join(model_folder, \"%inth_run\"%n_logs)\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=tensorboard_logs, write_graph=False)\n",
    "time_callback = TimeHistory()\n",
    "callbacks = [checkpoint, tensorboard_callback, time_callback]\n",
    "\n",
    "model = make_model((trainX.shape[1], trainX.shape[2]), trainY.shape[-1], CuDNNLSTM)\n",
    "\n",
    "model.fit(trainX, trainY, epochs=50, batch_size=512, sample_weight=weights, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.predict_classes(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 619,  278],\n",
       "       [ 404, 3348]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(testY.argmax(axis=1), output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8466336846633684"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output == testY.argmax(axis=1)).sum()/len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control accuracy: 0.75\n",
      "Infarct accuracy: 0.9701492537313433\n"
     ]
    }
   ],
   "source": [
    "summed = pd.DataFrame({'record':record_list, 'predictions':output, 'label':testY.argmax(axis=1)}).groupby('record').mean()\n",
    "control = summed.loc[summed['label'] == 0]\n",
    "print(\"Control accuracy: \"+  str((control['predictions'] <= 0.5).sum()/control.shape[0]))\n",
    "\n",
    "infarct = summed.loc[summed['label'] == 1]\n",
    "print(\"Infarct accuracy: \"+  str((infarct['predictions'] > 0.5).sum()/infarct.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x00000294A55AA978>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x00000294BBABA0B8>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "control.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "channel = channels[1]\n",
    "plt.plot(df_healthy.iloc[3][channel][0:window_size])\n",
    "plt.plot(df_myo.iloc[4][channel][0:window_size])\n",
    "plt.plot(df_dys.iloc[2][channel][0:window_size])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "record.label = comments_to_dict(record.comments)['Reason for admission']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Myocardial infarction'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Acute infarction (localization)': ' infero-latera',\n",
       " 'Additional diagnoses': ' Diabetes mellitus',\n",
       " 'Additional medication': ' Heparin Isosorbit-Mononitrate ASA Diazepam',\n",
       " 'Admission date': ' 29-Sep-90',\n",
       " 'Aorta (at rest) (syst/diast)': ' 160/64 cmH2O',\n",
       " 'Aorta (at rest) mean': ' 106 cmH2O',\n",
       " 'Cardiac index (at rest)': ' n/a',\n",
       " 'Cardiac index (load)': ' n/a',\n",
       " 'Cardiac output (at rest)': ' n/a',\n",
       " 'Cardiac output (load)': ' n/a',\n",
       " 'Catheterization date': ' 16-Oct-90',\n",
       " 'Chest X-ray': ' Heart size upper limit of norm',\n",
       " 'Diagnose': '',\n",
       " 'Dosage (lytic agent)': ' 30 mg',\n",
       " 'ECG date': ' 18/10/1990',\n",
       " 'Echocardiography': ' n/a',\n",
       " 'Former infarction (localization)': ' no',\n",
       " 'Hemodynamics': '',\n",
       " 'In hospital medication': ' ASA Isosorbit-Mononitrate Ca-antagonist Amiloride+Chlorothiazide Glibenclamide Insulin',\n",
       " 'Infarction date': ' 29-Sep-90',\n",
       " 'Infarction date (acute)': ' 29-Sep-90',\n",
       " 'Left coronary artery stenoses (RCX)': ' No stenoses',\n",
       " 'Left coronary artery stenoses (RIVA)': ' RIVA 70% proximal to ramus diagonalis_2',\n",
       " 'Left ventricular enddiastolic pressure': ' 11 cmH2O',\n",
       " 'Lytic agent': ' Gamma-TPA',\n",
       " 'Medication after discharge': ' ASA Isosorbit-Mononitrate Amiloride+Chlorothiazide Glibenclamide',\n",
       " 'Medication pre admission': ' Isosorbit-Dinitrate Digoxin Glibenclamide',\n",
       " 'Number of coronary vessels involved': ' 1',\n",
       " 'Peripheral blood Pressure (syst/diast)': '  140/80 mmHg',\n",
       " 'Previous infarction (1) date': ' n/a',\n",
       " 'Previous infarction (2) date': ' n/a',\n",
       " 'Pulmonary artery pressure (at rest) (mean)': ' n/a',\n",
       " 'Pulmonary artery pressure (at rest) (syst/diast)': ' n/a',\n",
       " 'Pulmonary artery pressure (laod) (mean)': ' n/a',\n",
       " 'Pulmonary artery pressure (laod) (syst/diast)': ' n/a',\n",
       " 'Pulmonary capillary wedge pressure (at rest)': ' n/a',\n",
       " 'Pulmonary capillary wedge pressure (load)': ' n/a',\n",
       " 'Reason for admission': ' Myocardial infarction',\n",
       " 'Right coronary artery stenoses (RCA)': ' No stenoses',\n",
       " 'Smoker': ' no',\n",
       " 'Start lysis therapy (hh.mm)': ' 19',\n",
       " 'Stroke volume index (at rest)': ' n/a',\n",
       " 'Stroke volume index (load)': ' n/a',\n",
       " 'Therapy': '',\n",
       " 'Ventriculography': ' Akinesia inferior wall',\n",
       " 'age': ' 81',\n",
       " 'sex': ' female'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_to_dict(record.comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading files...\n",
      "Finished downloading files\n"
     ]
    }
   ],
   "source": [
    "io.dl_database(db, 'data', records)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
